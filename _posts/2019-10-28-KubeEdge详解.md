---
layout:     post
title:      KubeEdge 详解
subtitle:   KubeEdge 的介绍 部署
date:       2019-10-28
author:     HY
header-img: img/articles/kubeedge/kubeedge.png
catalog: true
tags:
    - 边缘计算
    - kubernetes
    - kubeedge
---
# 一、KubeEdge介绍

KubeEdge 是一个开源的系统，可将本机容器化应用编排和管理扩展到边缘端设备。 它构建在Kubernetes之上，为网络和应用程序提供核心基础架构支持，并在云端和边缘端部署应用，同步元数据。100%兼容K8S API，可以使用K8S API原语管理边缘节点和设备。KubeEdge 还支持 MQTT 协议，允许开发人员编写客户逻辑，并在边缘端启用设备通信的资源约束。

## 1、优势
kubernetes + 容器的组合大大提高了用户创建部署应用的效率。kubernetes 可以把 n 台主机整合成一个集群，用户在 master 节点上通过编写一个 yaml 或者 json 格式的配置文件，也可以通过命令等请求 Kubernetes API 创建应用，就直接将应用部署到集群上的各个节点上，该配置文件中还包含了用户想要应用程序保持的状态，从而生成用户想要的环境。
Kubernetes 作为容器编排的标准，自然会想把它应用到边缘计算上，即通过 kubernetes 在边缘侧部署应用，但是 kubernetes 在边缘侧部署应用时遇到了一些问题，例如：
- 边缘侧设备没有足够的资源运行一个完整的 Kubelet
- 一些边缘侧设备是 ARM 架构的，然而大部分的 Kubernetes 发行版并不支持 ARM 架构
- 边缘侧网络很不稳定，甚至可能完全不通，而 kubernetes 需要实时通信，无法做到离线自治
- 很多边缘设备都不支持TCP/IP 协议
- Kubernetes 客户端（集群中的各个Node节点）是通过 list-watch 去监听 Master 节点的 apiserver 中资源的增删改查，list-watch 中的 watch 是调用资源的 watch API 监听资源变更事件，基于 HTTP 长连接实现，而维护一个 TCP 长连接开销较大。从而造成可扩展性受限。

为了解决包含但不限于以上 Kubernetes 在物联网边缘场景下的问题，从而产生了KubeEdge 。对应以上问题：
- KubeEdge 保留了 Kubernetes 的管理面，重新开发了节点 agent，大幅度优化让边缘组件资源占用更低很多
- KubeEdge 可以完美支持 ARM 架构和 x86 架构
- KubeEdge 有离线自治功能，可以看 MetaManager 组件的介绍
- KubeEdge 丰富了应用和协议支持，目前已经支持和计划支持的有：MQTT、BlueTooth、OPC UA、Modbus等。
- KubeEdge 通过底层优化的多路复用消息通道优化了云边的通信的性能，可以看 EdgeHub 组件的介绍


## 2、介绍

### 2.1 应用场景、特点等
![](/img/articles/kubeedge/1570603612250.png)

![](/img/articles/kubeedge/1570589799263.png)

上图是 [华为云IEF](https://www.huaweicloud.com/product/ief.html) 的应用场景，Kubeedge 就是源于这个产品，它基于 KubeEdge 和 Kubernetes 生态构建，将云原生的技术应用到边缘计算。IEF 通过纳管边缘节点，将云端AI应用、函数计算等能力下发到边缘节点（EdgeNode），将公有云能力延伸到靠近设备的一端，使得边缘节点拥有云端相同能力，能够实时处理终端设备计算需求。

[KubeConShanghai2018——KubeEdge开源首秀](https://zhuanlan.zhihu.com/p/75334170) 
[KubeEdge 向左，K3S 向右](https://zhuanlan.zhihu.com/p/61940924)
[KubeEdge实现原理](http://ljchen.net/2019/07/21/kubeedge%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/)
### 2.2 架构
![架构图](https://raw.githubusercontent.com/kubeedge/kubeedge/master/docs/images/kubeedge_arch.png)

KubeEdge 由以下组件构成:

#### 2.1.1 云边通信
- [CloudHub](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/cloudhub.md): CloudHub 是一个 Web Socket 服务端，用于大量的 edge 端基于 websocket 或者 quic 协议连接上来。**负责监听云端的变化, 缓存并发送消息到 EdgeHub**。
- [EdgeHub](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/edgehub.md): 是一个 Web Socket 客户端，负责将接收到的信息转发到各edge端的模块处理；同时将来自个edge端模块的消息通过隧道发送到cloud端。**提供可靠和高效的云边信息同步**。

[如何配置通信协议](https://github.com/kubeedge/kubeedge/blob/master/docs/proposals/quic-design.md#edgehub-connect-to-cloudhub-through-websocket-protocol)
#### 2.1.2 云上部分
- [EdgeController](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/controller.md): 用于控制 Kubernetes API Server 与边缘的节点、应用和配置的状态同步。
- [DeviceController](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/cloud/device_controller.md): DeviceController 是一个扩展的 Kubernetes 控制器，管理边缘设备，确保设备信息、设备状态的云边同步。

#### 2.1.3 边缘部分
- [MetaManager](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/metamanager.md): MetaManager 模块后端对应一个本地的数据库（sqlLite），所有其他模块需要与 cloud 端通信的内容都会被保存到本地 DB 种一份，当需要查询数据时，如果本地 DB 中存在该数据，就会从本地获取，这样就避免了与 cloud 端之间频繁的网络交互；同时，在网络中断的情况下，本地的缓存的数据也能够保障其稳定运行（比如你的智能汽车进入到没有无线信号的隧道中），在通信恢复之后，重新同步数据。是**边缘节点自治能力的关键**；
- [Edged](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/edged.md): 是运行在边缘节点的代理，用于管理容器化的应用程序。算是个重新开发的**轻量化 Kubelet**，实现 Pod，Volume，Node 等 Kubernetes 资源对象的生命周期管理
- [EventBus](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/eventbus.md): EventBus 是一个与 MQTT 服务器（mosquitto）交互的 MQTT 客户端，为其他组件提供订阅和发布功能。
- ServiceBus: ServiceBus是一个运行在边缘的HTTP客户端，接受来自云上服务的请求，与运行在边缘端的HTTP服务器交互，提供了云上服务通过HTTP协议访问边缘端HTTP服务器的能力。
- [DeviceTwin](https://github.com/kubeedge/kubeedge/blob/master/docs/modules/edge/devicetwin.md): DeviceTwin 负责存储设备状态并将设备状态同步到云，它还为应用程序提供查询接口。


# 二、kubEedge部署
## 1、先决条件
### 1.1 修改节点名称
```bash
hostnamectl set-hostname master
```
### [1.2 安装 docker](https://docs.docker.com/install/)
```bash
#关闭防火墙和selinux
systemctl stop firewalld && systemctl disable firewalld
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
#关闭swap
swapoff -a
yes | cp /etc/fstab /etc/fstab_bak
cat /etc/fstab_bak |grep -v swap > /etc/fstab

yum install yum-utils device-mapper-persistent-data lvm2

### Add Docker repository.
yum-config-manager \
  --add-repo \
  http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

## Install Docker CE.
yum update && yum install docker-ce-18.06.2.ce

## Create /etc/docker directory.
mkdir /etc/docker

# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart Docker
systemctl daemon-reload
systemctl restart docker
```
### [1.3 安装 kubeadm/kubectl](https://kubernetes.io/docs/setup/independent/install-kubeadm/)
```bash
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet-1.15.0 kubeadm-1.15.0 kubectl-1.15.0

systemctl enable kubelet && systemctl start kubelet
```
### [1.4 初始化 Kubernetes](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)
```bash
kubeadm init \
    --image-repository registry.aliyuncs.com/google_containers \
    --kubernetes-version $(kubeadm version -o short) \
    --pod-network-cidr=10.244.0.0/16
```
[初始化命令说明](https://k8smeetup.github.io/docs/admin/kubeadm/)
>集群初始化如果遇到问题，可以使用`kubeadm reset`命令进行清理然后重新执行初始化。
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
安装Pod网络插件(CNI)
```bash
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```
在完成 Kubernetes master 的初始化后， 我们需要暴露 Kubernetes apiserver 的 http 端口8080用于与 cloudcore/kubectl 交互。请按照以下步骤在 Kubernetes apiserver 中启用 http 端口。这样可以在边缘节点执行 `kubectl get nodes -s 192.169.0.10:8080`等命令，就像在 master 节点上一样。

```bash
vi /etc/kubernetes/manifests/kube-apiserver.yaml
# Add the following flags in spec: containers: -command section
- --insecure-port=8080
- --insecure-bind-address=0.0.0.0
```


### 1.5 克隆KubeEdge

```bash
git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge
cd $GOPATH/src/github.com/kubeedge/kubeedge
git checkout -b v1.0.0 v1.0.0
```

### 1.6 配置MQTT模式
KubeEdge 的边缘部分在 deviceTwin 和设备之间使用 MQTT 进行通信。KubeEdge 支持3个 MQTT 模式：
1) internalMqttMode: 启用内部  mqtt 代理。
2) bothMqttMode: 同时启用内部和外部代理。
3) externalMqttMode: 仅启用外部代理。

可以使用 [kubeedge/edge/conf/edge.yaml](https://github.com/kubeedge/kubeedge/blob/master/edge/conf/edge.yaml#L4) 中的 mode 字段去配置期望的模式。

使用 KubeEdge 的 mqtt 内部或外部模式，您都需要确保在边缘节点上安装 [mosquitto](https://mosquitto.org/) 或 [emqx edge](https://www.emqx.io/downloads/edge) 作为 MQTT Broker。

### 1.7 生成证书

KubeEdge 在云和边缘之间基于证书进行身份验证/授权。证书可以使用 openssl 生成。请按照以下步骤生成证书。

```bash
$GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh genCertAndKey edge
```

证书和密钥会分别自动生成在`/etc/kubeedge/ca` 和 `/etc/kubeedge/certs` 目录下。

## 2、运行KubeEdge

### 2.1 运行Cloud

#### 2.1.1 以二进制文件方式运行

- 构建 Cloud
    ```bash
    cd $GOPATH/src/github.com/kubeedge/kubeedge/cloud
    make # or `make cloudcore`
    ```
- 修改 `$GOPATH/src/github.com/kubeedge/kubeedge/cloud/conf/controller.yaml` 配置文件，将 `cloudhub.ca`、`cloudhub.cert`、`cloudhub.key`修改为生成的证书路径
  　
    ![Alt text](/img/articles/kubeedge/1569550469751.png)
　　
- 移除`$GOPATH/src/github.com/kubeedge/kubeedge/cloud/conf/modules.yaml`中的模块 `devicecontroller`，以免产生不必要的日志
　
- 创建 device model 和 device CRDs（可以不管）
    ```bash
    cd $GOPATH/src/github.com/kubeedge/kubeedge/build/crds/devices
    kubectl create -f devices_v1alpha1_devicemodel.yaml
    kubectl create -f devices_v1alpha1_device.yaml
    ```
- 运行二进制文件
    ```bash
    cd $GOPATH/src/github.com/kubeedge/kubeedge/cloud
    # run edge controller
    # `conf/` should be in the same directory as the cloned KubeEdge repository
    # verify the configurations before running cloud(cloudcore)
    ./edgecontroller
    ```

#### [2.1.2 以 k8s deployment 方式运行](https://github.com/kubeedge/kubeedge/blob/master/build/cloud/README_zh.md)

### 2.2 部署 Edge node
我们提供了一个示例 node.json 来在 Kubernetes 中添加一个节点。
请确保在 Kubernetes 中添加了边缘节点 edge-node。运行以下步骤以添加边缘节点 edge-node。

- 编辑 `$GOPATH/src/github.com/kubeedge/kubeedge/build/node.json` 文件，将 `metadata.name` 修改为edge node name
　
- 部署node
    ```bash
    kubectl apply -f $GOPATH/src/github.com/kubeedge/kubeedge/build/node.json
    ```
+ 将证书文件传输到edge node
    ```bash
    scp -r /etc/kubeedge/ca root@172.31.0.175:/etc/kubeedge
    scp -r /etc/kubeedge/certs root@172.31.0.175:/etc/kubeedge
    ```

### 2.3 运行Edge

##### 2.3.1 以二进制文件方式运行

+ 构建 Edge
  ```bash
  cd $GOPATH/src/github.com/kubeedge/kubeedge/edge
  make # or `make edgecore`
  ```
  KubeEdge 可以跨平台编译，运行在基于ARM的处理器上。
  请点击 [Cross Compilation](https://github.com/kubeedge/kubeedge/blob/master/docs/setup/cross-compilation.md) 获得相关说明。
         　
+ 修改`$GOPATH/src/github.com/kubeedge/kubeedge/edge/conf/edge.yaml`配置文件
  + 将 `edgehub.websocket.certfile` 和 `edgehub.websocket.keyfile` 替换为自己的证书路径
  + 将 `edgehub.websocket.url` 中的 `0.0.0.0` 修改为 master node 的IP
  + 用 edge node name 替换 yaml文件中的 `fb4eb70-2783-42b8-b3f-63e2fd6d242e`
          
+ 运行二进制文件
  ```bash
  # run mosquitto
  mosquitto -d -p 1883
  # or run emqx edge
  # emqx start
  
  # run edgecore
  # `conf/` should be in the same directory as the cloned KubeEdge repository
  # verify the configurations before running edge(edgecore)
  ./edge_core
  # or
  nohup ./edgecore > edgecore.log 2>&1 &
  ```

  请使用具有root权限的用户运行 edge。

##### [2.3.2 以容器方式运行](https://github.com/kubeedge/kubeedge/blob/master/build/edge/README_zh.md)
- 拉出 *Edgecore* 的 *docker* 镜像（本文使用的镜像是从release-1.0创建的）。
    ```bash
    docker pull kubeedge/edgecore:v1.0.0
    docker tag kubeedge/edgecore:v1.0.0 kubeedge/edgecore:latest
    ```
- 使用容器部署 *kubeedge* 的边缘节点：
    - `cloudhub`中`0.0.0.0`替换为运行云组件的 *ip*
    - `edgename`为创建的节点名称
    - `containername`为创建的容器名

    ```bash
    cd $GOPATH/src/github.com/kubeedge/kubeedge/build/edge
    #检查容器运行环境
    ./run_daemon.sh prepare
    
    ./run_daemon.sh only_run_edge mqtt=0.0.0.0:1883 \
    cloudhub=192.169.0.10:10000 \
    edgename=edge-node3 \
    image="kubeedge/edgecore:latest" \
    containername=edge-node3
    ```
##### [2.3.3 以 k8s deployment 方式运行](https://github.com/kubeedge/kubeedge/blob/master/build/edge/kubernetes/README_zh.md)

#### 2.4 检查状态
在 Cloud 和 Edge 被启动之后，通过如下的命令去检查边缘节点的状态。

```bash
kubectl get nodes
```

请确保您创建的边缘节点状态是 **ready**。

## 三、部署应用

像使用普通k8s一样部署你的应用到edge节点
示例：

```bash
kubectl apply -f $GOPATH/src/github.com/kubeedge/kubeedge/build/deployment.yaml
```

**提示：** 目前对于边缘端，必须在 Pod 配置中使用 hostPort，不然 Pod 会一直处于 ContainerCreating 状态。 hostPort 必须等于 containerPort 而且不能为 0。

然后可以使用下面的命令检查应用程序是否正常运行。

```bash
kubectl get pods
```

